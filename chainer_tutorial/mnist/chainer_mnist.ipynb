{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chainerで多層パーセプトロンを作ろう\n",
    "## MNISTデータで手書き文字認識\n",
    "\n",
    "このファイルは以下のGitHubディレクトリにあるtrain_mnist.pyをipython notebookにペタペタ貼ったもの。\n",
    "https://github.com/pfnet/chainer/tree/master/examples/mnist\n",
    "\n",
    "- [Chainer@GitHub](https://github.com/pfnet/chainer.git)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このipynbファイルを、chainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from chainer import Variable, Chain, optimizers\n",
    "import chainer.functions as F\n",
    "import chainer.links as L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = data.load_mnist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'target': array([5, 0, 4, ..., 4, 5, 6], dtype=uint8)}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist['data'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_all = mnist['data'].astype(np.float32) / 255\n",
    "y_all = mnist['target'].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, x_test = np.split(x_all, [60000])\n",
    "y_train, y_test = np.split(y_all, [60000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLP(Chain):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__(\n",
    "            l1 = L.Linear(784, 100),\n",
    "            l2 = L.Linear(100, 100),\n",
    "            l3 = L.Linear(100, 10),\n",
    "        )\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        y = self.l3(h2)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失関数と分類精度の計算をするために、MLP chainの上にclassifier chainを作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyClassifier(Chain):\n",
    "    \"\"\"\n",
    "    Compute accuracy and loss.\n",
    "    \n",
    "    Returns:\n",
    "        loss\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, predictor):\n",
    "        super(MyClassifier, self).__init__(\n",
    "            predictor=predictor\n",
    "        )\n",
    "    \n",
    "    def __call__(self, x, t):\n",
    "        y = self.predictor(x)\n",
    "        self.loss = F.softmax_cross_entropy(y, t)\n",
    "        self.accuracy = F.accuracy(y, t)\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今、定義したMyClassifierクラスと似たクラスが、chainer.links.Classifierに定義されているので、通常はそっちをつかうべし。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = MyClassifier(MLP())\n",
    "#model = L.Classifier(MLP())    # <- 本来ならこっちの方が良い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = optimizers.SGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 書き方１"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchsize = 100\n",
    "datasize = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "epoch 10\n",
      "epoch 11\n",
      "epoch 12\n",
      "epoch 13\n",
      "epoch 14\n",
      "epoch 15\n",
      "epoch 16\n",
      "epoch 17\n",
      "epoch 18\n",
      "epoch 19\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    print('epoch %d' % epoch)\n",
    "    indices = np.random.permutation(datasize)\n",
    "    for i in range(0, datasize, batchsize):\n",
    "        x = Variable(x_train[indices[i : i + batchsize]])\n",
    "        t = Variable(y_train[indices[i : i + batchsize]])\n",
    "        \n",
    "        optimizer.update(model, x, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 書き方２"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchsize = 100\n",
    "datasize = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "mean loss: 0.16905\t mean accuracy: 0.94960\n",
      "epoch 1\n",
      "mean loss: 0.16476\t mean accuracy: 0.95150\n",
      "epoch 2\n",
      "mean loss: 0.16115\t mean accuracy: 0.95200\n",
      "epoch 3\n",
      "mean loss: 0.15899\t mean accuracy: 0.95290\n",
      "epoch 4\n",
      "mean loss: 0.15248\t mean accuracy: 0.95430\n",
      "epoch 5\n",
      "mean loss: 0.14985\t mean accuracy: 0.95500\n",
      "epoch 6\n",
      "mean loss: 0.14653\t mean accuracy: 0.95650\n",
      "epoch 7\n",
      "mean loss: 0.14390\t mean accuracy: 0.95710\n",
      "epoch 8\n",
      "mean loss: 0.14173\t mean accuracy: 0.95880\n",
      "epoch 9\n",
      "mean loss: 0.13728\t mean accuracy: 0.95860\n",
      "epoch 10\n",
      "mean loss: 0.13546\t mean accuracy: 0.95980\n",
      "epoch 11\n",
      "mean loss: 0.13285\t mean accuracy: 0.96090\n",
      "epoch 12\n",
      "mean loss: 0.12969\t mean accuracy: 0.96120\n",
      "epoch 13\n",
      "mean loss: 0.12803\t mean accuracy: 0.96220\n",
      "epoch 14\n",
      "mean loss: 0.12677\t mean accuracy: 0.96290\n",
      "epoch 15\n",
      "mean loss: 0.12319\t mean accuracy: 0.96320\n",
      "epoch 16\n",
      "mean loss: 0.12123\t mean accuracy: 0.96430\n",
      "epoch 17\n",
      "mean loss: 0.12186\t mean accuracy: 0.96340\n",
      "epoch 18\n",
      "mean loss: 0.11771\t mean accuracy: 0.96430\n",
      "epoch 19\n",
      "mean loss: 0.11739\t mean accuracy: 0.96600\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    print('epoch %d' % epoch)\n",
    "    \n",
    "    # Compute test/validation error\n",
    "    sum_loss, sum_accuracy = 0, 0\n",
    "    for i in range(0, 10000, batchsize):\n",
    "        x = Variable(x_test[i : i + batchsize])\n",
    "        t = Variable(y_test[i : i + batchsize])\n",
    "        loss = model(x, t)\n",
    "        sum_loss += loss.data * batchsize\n",
    "        sum_accuracy += model.accuracy.data * batchsize\n",
    "\n",
    "    mean_loss = sum_loss / 10000\n",
    "    mean_accuracy = sum_accuracy / 10000\n",
    "\n",
    "    print(\"mean loss: %.5f\\t mean accuracy: %.5f\" % (mean_loss, mean_accuracy))    \n",
    "    \n",
    "    indices = np.random.permutation(datasize)\n",
    "    for i in range(0, datasize, batchsize):\n",
    "        x = Variable(x_train[indices[i : i + batchsize]])\n",
    "        t = Variable(y_train[indices[i : i + batchsize]])\n",
    "        \n",
    "        model.zerograds()\n",
    "        loss = model(x, t)\n",
    "        loss.backward()\n",
    "                \n",
    "        optimizer.update()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
